{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ed8d7c-ac96-4b33-9cb7-4fa2a823bb6e",
   "metadata": {},
   "source": [
    "# Remove unnecessary data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b048511d-3c3e-4328-a4a2-13c6b660d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a660ee78-ddc2-45ea-b684-c68fd0215c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Project_DE11_test\")\\\n",
    "        .master(\"spark://host-192-168-2-13-de1:7077\")\\\n",
    "        .config(\"spark.cores.max\", 4)\\\n",
    "        .config(\"spark.executor.memory\", \"6G\")\\\n",
    "        .config(\"spark.executor.cores\", 4)\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", False)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\")\\\n",
    "        .config(\"spark.driver.port\", 10000)\\\n",
    "        .config(\"spark.blockManager.port\", 10006)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark_context = spark_session.sparkContext\n",
    "#sql_context = SQLContext(spark_context)# read the json file into dataframe\n",
    "# spark_session = SparkSession\\\n",
    "#         .builder\\\n",
    "#         .master(\"spark://host-192-168-2-20-de1:7077\") \\\n",
    "#         .appName(\"Project_DE11\")\\\n",
    "#         .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "#         .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "#         .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "#         .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "#         .config(\"spark.executor.cores\",2)\\\n",
    "#         .config(\"spark.driver.port\",9999)\\\n",
    "#         .config(\"spark.blockManager.port\",10005)\\\n",
    "#         .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a36dd0a-6b74-4308-aee9-43be663c61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cores for another application!\n",
    "\n",
    "spark_context.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7913a69-c07d-4a17-8ed7-d946109a0876",
   "metadata": {},
   "source": [
    "# Selection of posts that fit criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c63ba-a260-4aa7-b31f-f61cd2ca7ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff12704-2de1-41e8-8d2c-859f4ae437ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cba3ddeb-c952-47ac-b25a-70d37240c332",
   "metadata": {},
   "source": [
    "# Analysis of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f65b51-66dd-44d0-8071-5ca72f41ea15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80b40c8d-230a-4b56-9430-5ac0894861c6",
   "metadata": {},
   "source": [
    "# Subreddits in which people prefer to write longer posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3694ad2-4181-47bc-bf0e-a1105d4b101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal InRelease[0m\n",
      "Get:3 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Hit:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports InRelease\n",
      "Get:5 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3156 kB]\n",
      "Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1175 kB]\n",
      "Fetched 4445 kB in 2s (1880 kB/s)                         \u001b[0m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "21 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n",
      "Requirement already satisfied: pyspark in /home/ubuntu/.local/lib/python3.8/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/ubuntu/.local/lib/python3.8/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: py4j in /home/ubuntu/.local/lib/python3.8/site-packages (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "# ------ Install the necessary environment ------\n",
    "!sudo apt update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "!pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a6497e-9e32-44e7-8fac-fd8a1773e994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/spark-3.5.1-bin-hadoop3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import findspark\n",
    "\n",
    "# make pyspark importable as a regular library.\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d9af1-2b9d-429b-8c67-ecfe1723aca5",
   "metadata": {},
   "source": [
    "### Initialize pyspark and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99342cbd-24ea-4832-81c3-48172c4d88da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from typing import List\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from google.colab import drive\n",
    "from pyspark.sql.functions import split, col, udf, avg\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "\n",
    "spark_session = SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"Ankur Roy\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "df = spark_session.read.json(\"hdfs://host-192-168-2-13-de1:9000/user/ubuntu/reduced_set.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2072e9-1084-4336-9f85-27f15de16bbd",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b08a64-4096-494e-a11d-4e29aacf2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count = df.count()\n",
    "\n",
    "print(\"Number of lines in English transcripts:\", line_count)\n",
    "\n",
    "selected_df = df.select(\"author\", \"content\", \"content_len\", \"summary\", \"summary_len\", \"subreddit\")\n",
    "\n",
    "length_filter_df = selected_df.filter((selected_df[\"content_len\"] > 50) & (selected_df[\"summary_len\"] > 10))\n",
    "\n",
    "num_long_posts = length_filter_df.count()\n",
    "\n",
    "print(\"Number of long posts\", num_long_posts)\n",
    "\n",
    "length_filter_df = length_filter_df.withColumn(\"summary_words\", split(col(\"summary\"), \" \"))\n",
    "length_filter_df = length_filter_df.withColumn(\"content_words\", split(col(\"content\"), \" \"))\n",
    "\n",
    "\n",
    "\n",
    "#------ Utilize User-Defined Function to compute the number of common words-------\n",
    "def count_common_words(summary_words, content_words):\n",
    "    return len(set(summary_words) & set(content_words))\n",
    "\n",
    "# Register the UDF\n",
    "count_common_words_udf = udf(count_common_words, IntegerType())\n",
    "\n",
    "# Count the number of common words\n",
    "# Common words refer to those words which both appear in the summary and the content of posts.\n",
    "\n",
    "length_filter_df = length_filter_df.withColumn(\"num_common_words\",\n",
    "                   count_common_words_udf(col(\"summary_words\"), col(\"content_words\")))\n",
    "\n",
    "# Show the DataFrame\n",
    "length_filter_df.show(truncate=False)\n",
    "average_common_words = length_filter_df.agg(avg(\"num_common_words\")).collect()[0][0]\n",
    "\n",
    "print(\"Average number of posts' common words:\", average_common_words)\n",
    "\n",
    "\n",
    "\n",
    "quality_df = length_filter_df.filter(col(\"num_common_words\") >= 10)\n",
    "\n",
    "print(quality_df.count())\n",
    "\n",
    "quality_df.show()\n",
    "\n",
    "average_common_words = quality_df.agg(avg(\"num_common_words\")).collect()[0][0]\n",
    "\n",
    "print(\"Average number of common words in posts with more than 10 common words:\", average_common_words)\n",
    "\n",
    "\n",
    "\n",
    "#------ List the top 20 subreddits in which prefer to write longer posts-------\n",
    "\n",
    "count_df = quality_df.groupBy(\"subreddit\").count()\n",
    "count_df = count_df.orderBy(col(\"count\").desc())\n",
    "# Show the DataFrame\n",
    "count_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
